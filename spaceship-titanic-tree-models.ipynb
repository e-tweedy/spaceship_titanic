{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/eamonntweedy/spaceship-titanic-tree-models?scriptVersionId=114226274\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Spaceship-titanic: predicting with tree models","metadata":{"tags":[]}},{"cell_type":"markdown","source":"In this notebook, we will do some data investigation, data pre-processing, feature investigation, and prediction with the Starship-Titanic Kaggle dataset.  The policies used in imputing missing data were inspired by this very helpful thread:\nhttps://www.kaggle.com/competitions/spaceship-titanic/discussion/315987","metadata":{}},{"cell_type":"markdown","source":"* [Downloading data](#download)\n* [Loading data, etc.](#loading)\n    - [Cabin and group features](#cabin_group)\n    - [Visualizing feature data](#graphs)\n* [Data cleanup](#cleanup)\n    - [Assumptions/rules for imputing missing data](#missing)\n    - [Imputing missing data](#fill)\n    - [Finishing up the feature engineering](#finish)\n* [Model baseline evaluation](#models)\n* [Selecting features](#features)\n    - [Selecting features using SequentialFeatureSelector()](#sfs)\n* [Hyperparameter tuning](#tuning)\n    - [BayesSearchCV](#bayes)\n        * [XGBoost](#bayes_x)\n        * [LightGBM](#bayes_l)\n        * [CatBoost](#bayes_c)\n* [Predicting and writing to submission file](#pred)\n        ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"download\"></a>\n## Downloading spaceship-titanic dataset","metadata":{"tags":[]}},{"cell_type":"markdown","source":"First, we download the spaceship-titanice dataset from Kaggle.  We use the kaggle library from fastai for convenience.","metadata":{}},{"cell_type":"code","source":"%%capture\n! pip install kaggle\n\nimport os\nfrom pathlib import Path\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nif iskaggle:\n    path = Path('../input/spaceship-titanic')\n#    !pip install -Uqq fastai\nelse:\n    import zipfile,kaggle\n    path = Path('spaceship-titanic')\n    kaggle.api.competition_download_cli(str(path))\n    zipfile.ZipFile(f'{path}.zip').extractall(path)","metadata":{"execution":{"iopub.status.busy":"2022-12-18T04:46:29.744376Z","iopub.execute_input":"2022-12-18T04:46:29.745678Z","iopub.status.idle":"2022-12-18T04:46:40.853192Z","shell.execute_reply.started":"2022-12-18T04:46:29.745619Z","shell.execute_reply":"2022-12-18T04:46:40.851657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"loading\"></a>\n## Loading data, initial feature engineering, and visualizing data","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Load in our training and test data as dataframes.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-12-18T04:46:49.262384Z","iopub.execute_input":"2022-12-18T04:46:49.262857Z","iopub.status.idle":"2022-12-18T04:46:49.941119Z","shell.execute_reply.started":"2022-12-18T04:46:49.262817Z","shell.execute_reply":"2022-12-18T04:46:49.940007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(path/'train.csv')\ndf_test = pd.read_csv(path/'test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-12-18T04:46:50.615132Z","iopub.execute_input":"2022-12-18T04:46:50.615842Z","iopub.status.idle":"2022-12-18T04:46:50.700391Z","shell.execute_reply.started":"2022-12-18T04:46:50.615808Z","shell.execute_reply":"2022-12-18T04:46:50.69905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"cabin_group\"></a>\n### Cabin and group features","metadata":{}},{"cell_type":"markdown","source":"We create several new features with the following data pre-processing function:\n1. We split 'Cabin' into its component parts 'CabinDeck', 'CabinNum', 'CabinSide'.\n2. We've harvested all that we can from 'Cabin', so we drop it.  The feature 'Name' doesn't appear to be immediately useful, so we drop that too.\n3. We splits off the 'Group' number from 'PassengerId'.","metadata":{}},{"cell_type":"code","source":"cat_feat = ['HomePlanet','CryoSleep','Destination','VIP','CabinDeck','CabinSide','Group']\nnum_feat = ['Age','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','CabinNum','Expenses']\n\ndef extract_cabin_group(df):\n    df[['CabinDeck','CabinNum','CabinSide']]= df.Cabin.str.split('/',expand=True)\n    df['CabinNum']=df['CabinNum'].astype(float)\n    df.drop(['Cabin','Name'],axis=1,inplace=True)\n    df['Group']=df.PassengerId.str.split('_').str[0].astype(float)\n\nextract_cabin_group(df)\nextract_cabin_group(df_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"graphs\"></a>\n### Visualzing feature data","metadata":{"tags":[]}},{"cell_type":"markdown","source":"We take a look at bar graphs of counts for each categorical variable in the training set, as well as counts of transported and non-transported passenger by categorical variables.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(6,2, figsize=(20,35))\nidx = 0\nfor col in cat_feat[:-1]:\n    sns.countplot(data=df, y=col, palette='magma', orient='h',\n                  ax=axes[idx][0]).set_title(f'Count of {col}', fontsize='16')\n    sns.countplot(data=df, y=col, palette='mako', orient='h',  hue='Transported',\n                  ax=axes[idx][1]).set_title(f'Count of {col} per transported', fontsize='16')\n    idx +=1\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to make some observation that will help us decide how to impute missing data, we also look at the relative breakdown of pairs of categorical features, both in the training and test data sets.","metadata":{}},{"cell_type":"code","source":"plot_cols = ['CabinDeck','Destination','VIP','CryoSleep','CabinSide']\nfig, axes = plt.subplots(5,4, figsize=(30,35))\nidx = 0\nfor col in plot_cols:\n    sns.countplot(data=df, y=col, palette='magma', orient='h',\n                  ax=axes[idx][0]).set_title(f'Count of {col} (train)', fontsize='16')\n    sns.countplot(data=df, y=col, palette='magma', orient='h',  hue='HomePlanet',\n                  ax=axes[idx][1]).set_title(f'Count of {col} per HomePlanet (train)', fontsize='16')\n    sns.countplot(data=df_test, y=col, palette='magma', orient='h',\n                  ax=axes[idx][2]).set_title(f'Count of {col} (test)', fontsize='16')\n    sns.countplot(data=df_test, y=col, palette='magma', orient='h',  hue='HomePlanet',\n                  ax=axes[idx][3]).set_title(f'Count of {col} per HomePlanet (test)', fontsize='16')\n    idx +=1\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_cols = ['CabinDeck','VIP','CryoSleep','CabinSide']\nfig, axes = plt.subplots(4,4, figsize=(30,35))\nidx = 0\nfor col in plot_cols:\n    sns.countplot(data=df, y=col, palette='magma', orient='h',\n                  ax=axes[idx][0]).set_title(f'Count of {col} (train)', fontsize='16')\n    sns.countplot(data=df, y=col, palette='magma', orient='h',  hue='Destination',\n                  ax=axes[idx][1]).set_title(f'Count of {col} per Destination (train)', fontsize='16')\n    sns.countplot(data=df_test, y=col, palette='magma', orient='h',\n                  ax=axes[idx][2]).set_title(f'Count of {col} (test)', fontsize='16')\n    sns.countplot(data=df_test, y=col, palette='magma', orient='h',  hue='Destination',\n                  ax=axes[idx][3]).set_title(f'Count of {col} per Destination (test)', fontsize='16')\n    idx +=1\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_cols = ['VIP','CabinDeck','CabinSide']\nfig, axes = plt.subplots(3,4, figsize=(30,35))\nidx = 0\nfor col in plot_cols:\n    sns.countplot(data=df, y=col, palette='magma', orient='h',\n                  ax=axes[idx][0]).set_title(f'Count of {col} (train)', fontsize='16')\n    sns.countplot(data=df, y=col, palette='magma', orient='h',  hue='CryoSleep',\n                  ax=axes[idx][1]).set_title(f'Count of {col} per CryoSleep (train)', fontsize='16')\n    sns.countplot(data=df_test, y=col, palette='magma', orient='h',\n                  ax=axes[idx][2]).set_title(f'Count of {col} (test)', fontsize='16')\n    sns.countplot(data=df_test, y=col, palette='magma', orient='h',  hue='CryoSleep',\n                  ax=axes[idx][3]).set_title(f'Count of {col} per CryoSleep (test)', fontsize='16')\n    idx +=1\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_cols = ['VIP','CabinDeck']\nfig, axes = plt.subplots(2, 4, figsize=(30,35))\nidx = 0\nfor col in plot_cols:\n    sns.countplot(data=df, y=col, palette='magma', orient='h',\n                  ax=axes[idx][0]).set_title(f'Count of {col} (train)', fontsize='16')\n    sns.countplot(data=df, y=col, palette='magma', orient='h',  hue='CabinSide',\n                  ax=axes[idx][1]).set_title(f'Count of {col} per CabinSide (train)', fontsize='16')\n    sns.countplot(data=df_test, y=col, palette='magma', orient='h',\n                  ax=axes[idx][2]).set_title(f'Count of {col} (test)', fontsize='16')\n    sns.countplot(data=df_test, y=col, palette='magma', orient='h',  hue='CabinSide',\n                  ax=axes[idx][3]).set_title(f'Count of {col} per CabinSide (test)', fontsize='16')\n    idx +=1\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above charts will help us to deduce some assumptions and rules for imputing missing data.  First, let's also take a look at how CabinNum relates to Group within each CabinDeck class.","metadata":{}},{"cell_type":"code","source":"decks = ['A','B','C','D','E','F','G']\nbill_cols = ['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']\nfig, axes = plt.subplots(7, 10, figsize=(35,35))\nfig.suptitle('Group vs. CabinNum in each CabinDeck class',fontsize='24')\nidx_1 = 0\nfor deck in decks:\n    idx_2 = 0\n    for col in bill_cols:\n        subset = df[df.CabinDeck==deck]\n        sns.scatterplot(data=subset, x='Group', y='CabinNum',ax=axes[idx_1][idx_2]).set_title(f'{deck} (train)', fontsize='16')\n        idx_2 += 1\n    for col in bill_cols:\n        subset = df_test[df_test.CabinDeck==deck]\n        sns.scatterplot(data=subset, x='Group', y='CabinNum',ax=axes[idx_1][idx_2]).set_title(f'{deck} (test)', fontsize='16')\n        idx_2 += 1\n    idx_2 =0\n    idx_1 +=1\nfig.tight_layout()\nfig.subplots_adjust(top=0.95)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that within each class, CabinDeck and Group are fairly close to being linearly related.  We will use this observation later - we shall use linear regression to fill the missing CabinNum fields.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"cleanup\"></a>\n## Data cleanup","metadata":{}},{"cell_type":"markdown","source":"<a id=\"missing\"></a>\n### Assumptions and rules for imputing missing data:\n\nWe describe some observations about missing data and the rules we use to impute.  Note that it can be helpful if the missing data isn't filled in too carefully - adding some noise to the training set can have a regularizing effect that can reduce chance of model overfitting the training set.\n\nMissing HomePlanet:\n1. If a passenger has CabinDeck in [A,B,C] then they are from Europa.\n2. If a passenger has CabinDeck G then they are from Earth\n3. If a passenger has CabinDeck D then they are from Europa or Mars.  We set to Mars (mode among these two).\n4. If a passenger has CabinDeck F then they are from Earth or Mars.  We set to Earth (mode among these two).\n5. If a passenger has CabinDeck E then can be from any - we set to Earth (mode among these)\n5. If a passenger is going to PSO or Trappist, we set to Earth (mode among these).\n6. If a passenter is going to Cancri, we set to Europa (mode among these).\n\nMissing CabinDeck: Just use HomePlanet\n1. If a passenger is from Earth, then set CabinDeck to G, the most likely.\n2. If a passenger is from Mars, then set CabinDeck to F, the most likely.\n3. If a passenger is from Europa, then they're roughly equally likely to be in B or C.  Just choose B arbitrarily.\n\nMissing CabinSide: Equally distributed between P and S - just choose S arbitrarily.\n\nMissing VIP: Almost nobody is VIP.  Fill in all missing VIP as False.\n\nMissing Cryosleep: We base this entirely on spending categories.\n1. If a passenger has nonzero spending in any category, then set CryoSleep = False.\n2. If a passenger has zero spending in all categories, we set CryoSleep=True (much more likely)\n\nMissing Destination: vast majority are going to Trappist, and there's no other category that would have likelier destination.\n\nMissing Age: Fill with mean age (though may be a more nuanced way to do this).\n\nMissing CabinNum: Seems linearly correlated with Group.  We fill in missing CabinNum using a linear regression prediction on group, within each CabinDeck class.\n\nMissing Bills: We fill all missing bills with 0.  Note that roughly half of those with missing bills have CryoSleep=True, so necessarily should have all bills 0.\n","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a id=\"fill\"></a>\n### Imputing missing data","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following block of functions will be used to fill in the missing data, according to the observations and rules above.","metadata":{}},{"cell_type":"code","source":"def fill_missing_home(df):\n    filter_ABC = (df.HomePlanet.isna())&((df.CabinDeck == 'A')|(df.CabinDeck == 'B')|(df.CabinDeck == 'C'))\n    filter_G_PSO_Trap = (df.HomePlanet.isna())&((df.CabinDeck == 'G')|(df.Destination == 'PSO J318.5-22')|(df.Destination == 'TRAPPIST-1e'))\n    filter_Can = (df.HomePlanet.isna())&(df.Destination == '55 Cancri e')\n    filter_D = (df.HomePlanet.isna())&(df.CabinDeck == 'D')\n    filter_EF = (df.HomePlanet.isna())&((df.CabinDeck == 'E')|(df.CabinDeck == 'F'))\n    df.loc[filter_ABC,'HomePlanet']='Europa'\n    df.loc[filter_G_PSO_Trap,'HomePlanet']='Earth'\n    df.loc[filter_Can,'HomePlanet']='Europa'\n    df.loc[filter_D,'HomePlanet']='Mars'\n    df.loc[filter_EF,'HomePlanet']='Earth'\n\ndef fill_missing_dest(df):\n    df.loc[df.Destination.isna(),'Destination']='TRAPPIST-1e'\n    \ndef fill_missing_deck(df):\n    filter_Earth = (df.CabinDeck.isna())&(df.HomePlanet == 'Earth')\n    filter_Mars = (df.CabinDeck.isna())&(df.HomePlanet == 'Mars')\n    df.loc[filter_Earth,'CabinDeck']='G'\n    df.loc[filter_Mars,'CabinDeck']='F'\n    df.loc[df.CabinDeck.isna(),'CabinDeck']='B'\n    \ndef fill_missing_side(df):\n    df.loc[df.CabinSide.isna(),'CabinSide']='S'\n    \ndef fill_missing_cryo(df):\n    bill_cols = ['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']\n    filter_bill = (df.CryoSleep.isna())&((df[bill_cols]>0).any(1))\n    df.loc[filter_bill,'CryoSleep'] = False\n    df.loc[df.CryoSleep.isna(),'CryoSleep']=True\n    \ndef fill_missing_bills(df):\n    bill_cols = ['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']\n    for col in bill_cols:\n        df.loc[df[col].isna(),col]=0\n\ndef fill_missing_cabin_num(df):\n    deck_labels = ['B','C','D','E','F','G']\n    for deck in deck_labels:\n        df_deck =df[df.CabinDeck==deck]\n        df_deck_ok = df_deck[df_deck.CabinNum.notnull()]\n        df_deck_nan = pd.DataFrame(df_deck[df_deck.CabinNum.isna()])\n        X_nan = df_deck_nan.loc[:,'Group'].values.astype(int).reshape(-1,1)\n        X = df_deck_ok.loc[:,'Group'].values.astype(int).reshape(-1,1)\n        Y = df_deck_ok.loc[:,'CabinNum'].values.astype(int).reshape(-1,1)\n        lr = LinearRegression()\n        lr.fit(X,Y)\n        df.loc[(df.CabinNum.isna())&(df.CabinDeck==deck),'CabinNum'] = df.Group.apply(lambda x:max(0,np.rint(lr.predict([[x]]).item())))\n    \ndef fill_missing_cats(df):\n    fill_missing_home(df)\n    df.loc[df.VIP.isna(),'VIP'] = False\n    fill_missing_dest(df)\n    fill_missing_side(df)\n    fill_missing_deck(df)\n    fill_missing_bills(df)\n    fill_missing_cryo(df)\n    mean_age = np.rint(df['Age'].mean()).astype(float)\n    df.loc[df.Age.isna(),'Age']=mean_age\n    fill_missing_cabin_num(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fill_missing_cats(df)\nfill_missing_cats(df_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Verify that we've filled in all missing data:","metadata":{}},{"cell_type":"code","source":"df.isna().sum().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.isna().sum().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"finishing\"></a>\n### Finishing up the feature engineering","metadata":{}},{"cell_type":"markdown","source":"We convert the categorical features to numerals by their category codes:","metadata":{}},{"cell_type":"code","source":"for data in [df,df_test]:\n    for feature in cat_feat:\n        data[feature] = pd.Categorical(data[feature])\n    data[cat_feat] = data[cat_feat].apply(lambda x: x.cat.codes)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Verify that dataframes look as we expect:","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that all expenses have been filled in, we create a new feature that is their sum:","metadata":{}},{"cell_type":"code","source":"for data in [df,df_test]:\n    data['Expenses']=data['RoomService']+data['FoodCourt']+data['ShoppingMall']+data['Spa']+data['VRDeck']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we build the dataframes we'll use for modeling going forward:","metadata":{}},{"cell_type":"code","source":"X = df[cat_feat+num_feat]\nX_test = df_test[cat_feat+num_feat]\ny = df['Transported'].astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"models\"></a>\n## Model baseline evaluation and model selection","metadata":{}},{"cell_type":"code","source":"%%capture\n! pip install catboost\n! pip install lightgbm\n! pip install xgboost\n! pip install scikit-optimize","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, mean_absolute_error\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, StackingClassifier\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom sklearn.linear_model import LogisticRegression\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We split into training and validation sets:","metadata":{}},{"cell_type":"code","source":"X_train,X_val,y_train,y_val = train_test_split(X,y,random_state=42,shuffle=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"We use Stratified KFold splits in our cross-validation throughout","metadata":{}},{"cell_type":"code","source":"skf = StratifiedKFold(shuffle=True,random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll begin with a baseline cross-validation for five different tree/forest models: RandomForestClassifier(), AdaBoostClassifier, XGBClassifier(), LGBMClassifier(), and CatBoostClassifier().","metadata":{}},{"cell_type":"code","source":"def cross_val(clf,name,features):\n    cvs=cross_val_score(clf,X[features],y,cv=skf)\n    print(f'{name} has CV scores: {cvs} with mean: {cvs.mean()}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_all = cat_feat+num_feat","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc = RandomForestClassifier()\nabc = AdaBoostClassifier()\nxgbc = XGBClassifier(verbosity=0)\nlgbmc = LGBMClassifier()\ncbc = CatBoostClassifier(verbose=False)\nclf_list = [(rfc,'Random Forest'),(abc,'AdaBoost'),(xgbc,'XGBoost'),(lgbmc,'LightGBM'),(cbc,'CatBoost')]\n\nfor clf in clf_list:\n    cross_val(clf[0],clf[1],feat_all)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CV scores off the shelf look strong from CatBoostClassifier() and LGBMClassifier(), whereas the other three are a little worse.  We will move forward with XGBoost, LightGBM, and CatBoost.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"features\"></a>\n## Selecting features","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sfs\"></a>\n### Best feature sets using SequentialFeatureSelector()","metadata":{}},{"cell_type":"markdown","source":"We shall use scikit-learn SequentialFeatureSelector to identify best feature lists.  SequentialFeatureSelection works with a particular model, and so potentially gives different feature lists for different models.  We shall use XGBoost for this selection, one may want to in principle execute different feature selection processes for different models.","metadata":{}},{"cell_type":"markdown","source":"#### Finding N-best feature lists for XGBoost, N = 14,...,10","metadata":{}},{"cell_type":"markdown","source":"We save the resulting feature lists from SequentialFeatureSelector() in a dictionary 'sfs_dict'.  Entries of sfs_dict are of the form N:[list of N best features].","metadata":{}},{"cell_type":"code","source":"xgbc = XGBClassifier()\n\nsfs_dict = {}\nfeat_all = np.array(cat_feat+num_feat)\nfor n in [14,13,12,11,10]:\n    sfs = SequentialFeatureSelector(xgbc, n_features_to_select=n,direction='backward',cv=skf)\n    sfs.fit(X,y)\n    sfs.get_feature_names_out\n    best_feat =  feat_all[sfs.get_support()]\n    print(f'Best {n} features: {best_feat}')\n    sfs_dict[n]=best_feat","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we compute CV scores for XGBoost, LightGBM, and CatBoost using feature lists of various sizes.","metadata":{}},{"cell_type":"code","source":"xgbc = XGBClassifier(verbosity=0)\nlgbmc = LGBMClassifier()\ncbc = CatBoostClassifier(verbose=False)\n\nclf_list = [(xgbc,'XGBoost'),(lgbmc,'LightGBM'),(cbc,'CatBoost')]\n\nfor num in sfs_dict:\n    print(f'CV scores for {num} features: \\n ---------- \\n')\n    for clf in clf_list:\n        cross_val(clf[0],clf[1],sfs_dict[num])\n    print('========== \\n')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Those are pretty good, with the best 14 list giving strongest CV accuracy scores among feature lists and CatBoost giving the strongest CV accuracy scores among classifiers.  Going forward we will use the best 14 features.  We will tune hyperparameters for all three classifiers, but I expect CatBoost will give the best results overall.","metadata":{}},{"cell_type":"code","source":"best_14=sfs_dict[14]","metadata":{"execution":{"iopub.status.busy":"2022-12-18T12:52:24.289677Z","iopub.execute_input":"2022-12-18T12:52:24.290146Z","iopub.status.idle":"2022-12-18T12:52:24.306664Z","shell.execute_reply.started":"2022-12-18T12:52:24.290112Z","shell.execute_reply":"2022-12-18T12:52:24.304868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"tuning\"></a>\n## Hyperparameter tuning XGBoost, LightGBM, and CatBoost","metadata":{"tags":[]}},{"cell_type":"code","source":"%%capture\n! pip install scikit-optimize\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, KFold\nfrom sklearn.metrics import accuracy_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We create some hyperparameter spaces for use in our BayesSearchCV searches:","metadata":{}},{"cell_type":"markdown","source":"<a id=\"bayes\"></a>\n### BayesSearchCV","metadata":{}},{"cell_type":"code","source":"xgb = XGBClassifier(verbosity = 0)\nparams_xgb = {\n    'n_estimators':Integer(50,200),\n    'learning_rate':Real(0.01, 0.5,'log-uniform'),\n    'max_depth':Integer(1, 12),\n    'subsample':Real(0.1, 1.0,'uniform'),\n    'colsample_bytree':Real(0.5,1.0,'uniform'),\n    'colsample_bylevel':Real(0.5,1.0,'uniform'),\n    'colsample_bynode':Real(0.5,1.0,'uniform'),\n    'reg_lambda':Real(0,10,'uniform'),\n    'reg_alpha':Real(0,10,'uniform'),\n    'num_parallel_tree':Integer(1,10),\n    'min_child_weight':Real(0,10,'uniform'),\n    'min_child_samples':Integer(10,50)\n}\n\nopt_xgb = BayesSearchCV(xgb,search_spaces=params_xgb,verbose=1,cv=skf,n_jobs=5, n_iter=100, random_state=42)\n\nlgb = LGBMClassifier(verbosity = -1)\nparams_lgb = {\n    'n_estimators':Integer(50,300),\n    'learning_rate':Real(0.01, 0.5,'log-uniform'),\n    'max_depth':Integer(1, 12),\n    'num_leaves':Integer(2,2000,'log-uniform'),\n    'subsample':Real(0.1, 1.0,'uniform'),\n    'min_child_weight':Real(1e-3,10,'uniform'),\n    'min_child_samples':Integer(10,50),\n    'colsample_bytree':Real(0.5,1.0,'uniform'),\n    'reg_lambda':Real(0,10,'uniform')\n}\nopt_lgb = BayesSearchCV(lgb,search_spaces=params_lgb,verbose=1,cv=skf,n_jobs=5, n_iter=100, random_state=42)\n\ncb = CatBoostClassifier(verbose=False)\nparams_cb = {\n    'n_estimators':Integer(600,1200),\n    'depth':Integer(1, 12),\n    'subsample':Real(0.1, 1.0,'uniform'),\n    'random_strength': Real(1e-9, 10, 'log-uniform'),\n    'bagging_temperature': Real(0.0, 1.0),\n    'rsm':Real(0.5,1.0,'uniform'),\n    'l2_leaf_reg':Real(1,30,'uniform')\n}\nopt_cb = BayesSearchCV(cb,search_spaces=params_cb,verbose=1,cv=skf, n_jobs=5, n_iter=100, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-12-18T12:42:18.429559Z","iopub.execute_input":"2022-12-18T12:42:18.430108Z","iopub.status.idle":"2022-12-18T12:42:18.532726Z","shell.execute_reply.started":"2022-12-18T12:42:18.429977Z","shell.execute_reply":"2022-12-18T12:42:18.531405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"bayes_x\"></a>\n#### BayesSearchCV for XGBoost","metadata":{}},{"cell_type":"code","source":"opt_xgb.fit(X[best_14],y)\nprint(f'Results for XGBoost ---> Best params: {opt_xgb.best_params_} \\n Best score: {opt_xgb.best_score_}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"bayes_l\"></a>\n#### BayesSearchCV for LightGBM","metadata":{"tags":[]}},{"cell_type":"code","source":"opt_lgb.fit(X[best_14],y)\nprint(f'Results for LightGBM ---> Best params: {opt_lgb.best_params_} \\n Best score: {opt_lgb.best_score_}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"bayes_c\"></a>\n#### BayesSearchCV for CatBoost","metadata":{}},{"cell_type":"code","source":"opt_cb.fit(X[best_14],y)\nprint(f'Results for CatBoost ---> Best params: {opt_cb.best_params_} \\n Best score: {opt_cb.best_score_}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We train CatBoostClassifier with the hyperparameters identified by BayesSearchCV:","metadata":{}},{"cell_type":"code","source":"xgbc = XGBClassifier(verbosity = 0,colsample_bylevel=1,colsample_bynode=0.5686848205241387,colsample_bytree=1, learning_rate=0.33326322321728113,max_depth=12,min_child_samples=50,min_child_weight=0,n_estimators=79,num_parallel_tree=10, reg_alpha=10,reg_lambda=10, subsample=0.8287935953498862)\nlgbmc = LGBMClassifier(colsample_bytree=1,learning_rate=0.0574159608617656,max_depth=11,min_child_samples=19,min_child_weight=2.6513926370045096,n_estimators=300,num_leaves=46,reg_lambda=9.733031038389,subsample=0.1)\ncbc = CatBoostClassifier(verbose=False,bagging_temperature=0.5356585211512814, depth=7,l2_leaf_reg=1,n_estimators=1200,random_strength=4.810440963747201,rsm=0.5,subsample=0.702836334143052)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_list = [('XGBoost',xgbc),('LightGBM',lgbmc),('CatBoost',cbc)]\nfor clf in clf_list:\n    cross_val(clf[1],clf[0],best_14)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CatBoost has the strongest CV scores, and so we train and predict using it:","metadata":{}},{"cell_type":"code","source":"def train_predict(clf,features):\n    clf.fit(X[features],y)\n    preds = clf.predict(X_test[features])\n    return preds","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds=train_predict(cb,best_14)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def subm(preds, suff):\n    df_test['Transported'] = preds.astype(bool)\n    sub_df = df_test[['PassengerId','Transported']]\n    sub_df.to_csv(f'sub-{suff}.csv', index=False)\n\nsubm(preds, 'CatBoost with 14 best features from backward SFS, and hyperparameters optimized using BayesSearchCV')","metadata":{},"execution_count":null,"outputs":[]}]}